{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385c49a5-ac6e-4545-9780-96deb25c64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "import torch.nn.functional as F\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# suppress warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import scipy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "#import seaborn as sns\n",
    "#sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c282c",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6e1b23-ee8d-4dcb-8ff3-6974be44c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Define the path to the data folder\n",
    "data_folder = os.path.join(current_directory, 'data')\n",
    "\n",
    "# List all files in the data folder\n",
    "files = os.listdir(data_folder)\n",
    "\n",
    "\n",
    "# Read each CSV file in the data folder\n",
    "for file in files:\n",
    "    if file.endswith('listings2024_03.csv'):\n",
    "        print(\"df2024_03\")\n",
    "        file_path1 = os.path.join(data_folder, file)\n",
    "        df2024_03 = pd.read_csv(file_path1)\n",
    "    elif file.endswith('listings2024_02.csv'):\n",
    "        print(\"df2024_02\")\n",
    "        file_path2 = os.path.join(data_folder, file)\n",
    "        df2024_02 = pd.read_csv(file_path2)\n",
    "    elif file.endswith(\"listings2024_01.csv\"):\n",
    "        print(\"df2024_01\")\n",
    "        file_path3 = os.path.join(data_folder, file)\n",
    "        df2024_01 = pd.read_csv(file_path3)\n",
    "    elif file.endswith(\"listings2023_12.csv\"):\n",
    "        print(\"df2023_12\")\n",
    "        file_path4 = os.path.join(data_folder, file)\n",
    "        df2023_12 = pd.read_csv(file_path4)\n",
    "    elif file.endswith(\"listings2023_11.csv\"):\n",
    "        print(\"df2023_11\")\n",
    "        file_path5 = os.path.join(data_folder, file)\n",
    "        df2023_11 = pd.read_csv(file_path5)\n",
    "    elif file.endswith(\"listings2023_10.csv\"):\n",
    "        print(\"df2023_10\")\n",
    "        file_path6 = os.path.join(data_folder, file)\n",
    "        df2023_10 = pd.read_csv(file_path6)\n",
    "    elif file.endswith(\"listings2023_09.csv\"):\n",
    "        print(\"df2023_09\")\n",
    "        file_path7 = os.path.join(data_folder, file)\n",
    "        df2023_09 = pd.read_csv(file_path7)\n",
    "    elif file.endswith(\"listings2023_08.csv\"):\n",
    "        print(\"df2023_08\")\n",
    "        file_path8 = os.path.join(data_folder, file)\n",
    "        df2023_08 = pd.read_csv(file_path8)\n",
    "    elif file.endswith(\"listings2023_07.csv\"):\n",
    "        print(\"df2023_07\")\n",
    "        file_path9 = os.path.join(data_folder, file)\n",
    "        df2023_07 = pd.read_csv(file_path9)        \n",
    "    elif file.endswith(\"listings2023_06.csv\"):\n",
    "        print(\"df2023_06\")\n",
    "        file_path10 = os.path.join(data_folder, file)\n",
    "        df2023_06 = pd.read_csv(file_path10)        \n",
    "    elif file.endswith(\"listings2023_05.csv\"):\n",
    "        print(\"df2023_05\")\n",
    "        file_path11 = os.path.join(data_folder, file)\n",
    "        df2023_05 = pd.read_csv(file_path11)\n",
    "    elif file.endswith(\"listings2023_04.csv\"):\n",
    "        print(\"df2023_04\")\n",
    "        file_path12 = os.path.join(data_folder, file)\n",
    "        df2023_04 = pd.read_csv(file_path12)\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b8ee4d1-e043-4c13-95f3-c19f08f935de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of DataFrames\n",
    "dataframes = [\n",
    "    df2023_08, df2024_01, df2023_09, df2024_02, df2024_03,\n",
    "    df2023_07, df2023_12, df2023_06, df2023_10, df2023_04,\n",
    "    df2023_05, df2023_11\n",
    "]\n",
    "\n",
    "# Dictionary to hold the names and lengths\n",
    "df_lengths = {}\n",
    "\n",
    "# Iterate through the DataFrames and print their lengths\n",
    "for i, df in enumerate(dataframes, start=4):\n",
    "    month_year = f\"df2023_{i:02}\" if i < 13 else f\"df2024_{i - 12:02}\"\n",
    "    df_lengths[month_year] = len(df)\n",
    "\n",
    "# Print the lengths\n",
    "for name, length in df_lengths.items():\n",
    "    print(f\"Length of {name}: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3315be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_values = {}\n",
    "\n",
    "for i, df in enumerate(dataframes, start=4):\n",
    "    month_year = f\"df2023_{i:02}\" if i < 13 else f\"df2024_{i - 12:02}\"\n",
    "    df_missing_values[month_year] = df.isnull().sum()\n",
    "\n",
    "for name, missing_counts in df_missing_values.items():\n",
    "    print(f\"Missing values in {name}:\")\n",
    "    print(missing_counts)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a645709c",
   "metadata": {},
   "source": [
    "### Add Zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddbb443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NYCzipcode = gpd.read_file('./Data/ZIPCODE/ZIP_CODE_040114.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c87dc57e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Length of df2023_04: 43582\n",
    "Length of df2023_05: 39719\n",
    "Length of df2023_06: 39453\n",
    "Length of df2023_07: 39202\n",
    "Length of df2023_08: 39319\n",
    "Length of df2023_09: 43729\n",
    "Length of df2023_10: 39627\n",
    "Length of df2023_11: 43566\n",
    "Length of df2023_12: 38792\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f26217eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2023_04_gdf = gpd.GeoDataFrame(\n",
    "    df2023_04,\n",
    "    geometry=gpd.points_from_xy(df2023_04.longitude, df2023_04.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98bb4020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2023_05_gdf = gpd.GeoDataFrame(\n",
    "    df2023_05,\n",
    "    geometry=gpd.points_from_xy(df2023_05.longitude, df2023_05.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d732c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2023_06_gdf = gpd.GeoDataFrame(\n",
    "    df2023_06,\n",
    "    geometry=gpd.points_from_xy(df2023_06.longitude, df2023_06.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb77483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2023_07_gdf = gpd.GeoDataFrame(\n",
    "    df2023_07,\n",
    "    geometry=gpd.points_from_xy(df2023_07.longitude, df2023_07.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "866ed493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2023_08_gdf = gpd.GeoDataFrame(\n",
    "    df2023_08,\n",
    "    geometry=gpd.points_from_xy(df2023_08.longitude, df2023_08.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "180a74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2023_09_gdf = gpd.GeoDataFrame(\n",
    "    df2023_09,\n",
    "    geometry=gpd.points_from_xy(df2023_09.longitude, df2023_09.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "111aadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2023_10_gdf = gpd.GeoDataFrame(\n",
    "    df2023_10,\n",
    "    geometry=gpd.points_from_xy(df2023_10.longitude, df2023_10.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52868ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2023_11_gdf = gpd.GeoDataFrame(\n",
    "    df2023_11,\n",
    "    geometry=gpd.points_from_xy(df2023_11.longitude, df2023_11.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb3a0fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2023_12_gdf = gpd.GeoDataFrame(\n",
    "    df2023_12,\n",
    "    geometry=gpd.points_from_xy(df2023_12.longitude, df2023_12.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e4a1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2024_01_gdf = gpd.GeoDataFrame(\n",
    "    df2024_01,\n",
    "    geometry=gpd.points_from_xy(df2024_01.longitude, df2024_01.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bf2a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2024_02_gdf = gpd.GeoDataFrame(\n",
    "    df2024_02,\n",
    "    geometry=gpd.points_from_xy(df2024_02.longitude, df2024_02.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4628a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2024_03_gdf = gpd.GeoDataFrame(\n",
    "    df2024_03,\n",
    "    geometry=gpd.points_from_xy(df2024_03.longitude, df2024_03.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e37914d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024_1 with ziocode not all data.\n",
    "df2024_01_gdf = df2024_01_gdf.to_crs(NYCzipcode.crs)\n",
    "joined_gdf = gpd.sjoin(df2024_01_gdf, NYCzipcode, how=\"left\", op=\"within\")\n",
    "unique_zipcodes = joined_gdf.reset_index().groupby('index').first()\n",
    "airbnb202401_with_zip = df2024_01.merge(unique_zipcodes[['ZIPCODE']], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b562758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023_4 with ziocode not all data.\n",
    "df2023_04_gdf = df2023_04_gdf.to_crs(NYCzipcode.crs)\n",
    "joined_gdf = gpd.sjoin(df2023_04_gdf, NYCzipcode, how=\"left\", op=\"within\")\n",
    "unique_zipcodes = joined_gdf.reset_index().groupby('index').first()\n",
    "airbnb202304_with_zip = df2023_04.merge(unique_zipcodes[['ZIPCODE']], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e220234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023_5 with ziocode not all data.\n",
    "df2023_05_gdf = df2023_05_gdf.to_crs(NYCzipcode.crs)\n",
    "joined_gdf = gpd.sjoin(df2023_05_gdf, NYCzipcode, how=\"left\", op=\"within\")\n",
    "unique_zipcodes = joined_gdf.reset_index().groupby('index').first()\n",
    "airbnb202305_with_zip = df2023_05.merge(unique_zipcodes[['ZIPCODE']], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "989d0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023_6 with ziocode not all data.\n",
    "df2023_06_gdf = df2023_06_gdf.to_crs(NYCzipcode.crs)\n",
    "joined_gdf = gpd.sjoin(df2023_06_gdf, NYCzipcode, how=\"left\", op=\"within\")\n",
    "unique_zipcodes = joined_gdf.reset_index().groupby('index').first()\n",
    "airbnb202306_with_zip = df2023_06.merge(unique_zipcodes[['ZIPCODE']], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7942575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023_7 with ziocode not all data.\n",
    "df2023_07_gdf = df2023_07_gdf.to_crs(NYCzipcode.crs)\n",
    "joined_gdf = gpd.sjoin(df2023_07_gdf, NYCzipcode, how=\"left\", op=\"within\")\n",
    "unique_zipcodes = joined_gdf.reset_index().groupby('index').first()\n",
    "airbnb202307_with_zip = df2023_07.merge(unique_zipcodes[['ZIPCODE']], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25a2383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023_8 with ziocode not all data.\n",
    "df2023_08_gdf = df2023_08_gdf.to_crs(NYCzipcode.crs)\n",
    "joined_gdf = gpd.sjoin(df2023_08_gdf, NYCzipcode, how=\"left\", op=\"within\")\n",
    "unique_zipcodes = joined_gdf.reset_index().groupby('index').first()\n",
    "airbnb202308_with_zip = df2023_08.merge(unique_zipcodes[['ZIPCODE']], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc6642d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023_9 with ziocode not all data.\n",
    "df2023_09_gdf = df2023_09_gdf.to_crs(NYCzipcode.crs)\n",
    "joined_gdf = gpd.sjoin(df2023_09_gdf, NYCzipcode, how=\"left\", op=\"within\")\n",
    "unique_zipcodes = joined_gdf.reset_index().groupby('index').first()\n",
    "airbnb202309_with_zip = df2023_09.merge(unique_zipcodes[['ZIPCODE']], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85388e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023_10 with ziocode not all data.\n",
    "df2023_10_gdf = df2023_10_gdf.to_crs(NYCzipcode.crs)\n",
    "joined_gdf = gpd.sjoin(df2023_10_gdf, NYCzipcode, how=\"left\", op=\"within\")\n",
    "unique_zipcodes = joined_gdf.reset_index().groupby('index').first()\n",
    "airbnb202310_with_zip = df2023_10.merge(unique_zipcodes[['ZIPCODE']], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8486079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023_11 with ziocode not all data.\n",
    "df2023_11_gdf = df2023_11_gdf.to_crs(NYCzipcode.crs)\n",
    "joined_gdf = gpd.sjoin(df2023_11_gdf, NYCzipcode, how=\"left\", op=\"within\")\n",
    "unique_zipcodes = joined_gdf.reset_index().groupby('index').first()\n",
    "airbnb202311_with_zip = df2023_11.merge(unique_zipcodes[['ZIPCODE']], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4943a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023_12 with ziocode not all data.\n",
    "df2023_12_gdf = df2023_12_gdf.to_crs(NYCzipcode.crs)\n",
    "joined_gdf = gpd.sjoin(df2023_12_gdf, NYCzipcode, how=\"left\", op=\"within\")\n",
    "unique_zipcodes = joined_gdf.reset_index().groupby('index').first()\n",
    "airbnb202312_with_zip = df2023_12.merge(unique_zipcodes[['ZIPCODE']], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9dfa871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024_2 with ziocode not all data.\n",
    "df2024_02_gdf = df2024_02_gdf.to_crs(NYCzipcode.crs)\n",
    "joined_gdf = gpd.sjoin(df2024_02_gdf, NYCzipcode, how=\"left\", op=\"within\")\n",
    "unique_zipcodes = joined_gdf.reset_index().groupby('index').first()\n",
    "airbnb202402_with_zip = df2024_02.merge(unique_zipcodes[['ZIPCODE']], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c436c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024_3 with ziocode not all data.\n",
    "df2024_03_gdf = df2024_03_gdf.to_crs(NYCzipcode.crs)\n",
    "joined_gdf = gpd.sjoin(df2024_03_gdf, NYCzipcode, how=\"left\", op=\"within\")\n",
    "unique_zipcodes = joined_gdf.reset_index().groupby('index').first()\n",
    "airbnb202403_with_zip = df2024_03.merge(unique_zipcodes[['ZIPCODE']], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a7753bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Removing the Duplicates if any\n",
    "airbnb202401_with_zip.duplicated().sum()\n",
    "airbnb202401_with_zip.drop_duplicates(inplace=True)\n",
    "len(airbnb202401_with_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47e06ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Removing the Duplicates if any\n",
    "airbnb202402_with_zip.duplicated().sum()\n",
    "airbnb202402_with_zip.drop_duplicates(inplace=True)\n",
    "len(airbnb202402_with_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b7358ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Duplicates if any\n",
    "airbnb202403_with_zip.duplicated().sum()\n",
    "airbnb202403_with_zip.drop_duplicates(inplace=True)\n",
    "len(airbnb202403_with_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05af36de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Duplicates if any\n",
    "airbnb202304_with_zip.duplicated().sum()\n",
    "airbnb202304_with_zip.drop_duplicates(inplace=True)\n",
    "len(airbnb202304_with_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b728613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Duplicates if any\n",
    "airbnb202305_with_zip.duplicated().sum()\n",
    "airbnb202305_with_zip.drop_duplicates(inplace=True)\n",
    "len(airbnb202305_with_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96f522d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Duplicates if any\n",
    "airbnb202306_with_zip.duplicated().sum()\n",
    "airbnb202306_with_zip.drop_duplicates(inplace=True)\n",
    "len(airbnb202306_with_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12d8cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Duplicates if any\n",
    "airbnb202307_with_zip.duplicated().sum()\n",
    "airbnb202307_with_zip.drop_duplicates(inplace=True)\n",
    "len(airbnb202307_with_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e99e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Duplicates if any\n",
    "airbnb202308_with_zip.duplicated().sum()\n",
    "airbnb202308_with_zip.drop_duplicates(inplace=True)\n",
    "len(airbnb202308_with_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2415a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Duplicates if any\n",
    "airbnb202309_with_zip.duplicated().sum()\n",
    "airbnb202309_with_zip.drop_duplicates(inplace=True)\n",
    "len(airbnb202309_with_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf1eae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Duplicates if any\n",
    "airbnb202310_with_zip.duplicated().sum()\n",
    "airbnb202310_with_zip.drop_duplicates(inplace=True)\n",
    "len(airbnb202310_with_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd9736e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Duplicates if any\n",
    "airbnb202311_with_zip.duplicated().sum()\n",
    "airbnb202311_with_zip.drop_duplicates(inplace=True)\n",
    "len(airbnb202311_with_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06ac3599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Duplicates if any\n",
    "airbnb202312_with_zip.duplicated().sum()\n",
    "airbnb202312_with_zip.drop_duplicates(inplace=True)\n",
    "len(airbnb202312_with_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8077b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb202401_with_zip.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd2ba6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb202402_with_zip.columns == airbnb202401_with_zip.columns \n",
    "airbnb202402_with_zip.columns == airbnb202403_with_zip.columns\n",
    "airbnb202402_with_zip.columns == airbnb202304_with_zip.columns\n",
    "airbnb202402_with_zip.columns == airbnb202305_with_zip.columns\n",
    "airbnb202402_with_zip.columns == airbnb202306_with_zip.columns\n",
    "airbnb202402_with_zip.columns == airbnb202307_with_zip.columns\n",
    "airbnb202402_with_zip.columns == airbnb202308_with_zip.columns\n",
    "airbnb202402_with_zip.columns == airbnb202309_with_zip.columns\n",
    "airbnb202402_with_zip.columns == airbnb202310_with_zip.columns\n",
    "airbnb202402_with_zip.columns == airbnb202311_with_zip.columns\n",
    "airbnb202402_with_zip.columns == airbnb202312_with_zip.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89cf9d",
   "metadata": {},
   "source": [
    "### Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "118c21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_crimes = os.path.join(data_folder, 'NYPD_Hate_Crimes_20240305.csv')\n",
    "crimes = pd.read_csv(file_path_crimes)\n",
    "crimes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d6eb1",
   "metadata": {},
   "source": [
    "## Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb67f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29617010",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.County.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54828439",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb202401_with_zip.neighbourhood_group.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7b20047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kings= Brooklyn, RichMond= Staten Island\n",
    "county_to_borough = {\n",
    "    'KINGS': 'Brooklyn',\n",
    "    'NEW YORK': 'Manhattan',\n",
    "    'QUEENS': 'Queens',\n",
    "    'BRONX': 'Bronx',\n",
    "    'RICHMOND': 'Staten Island'\n",
    "}\n",
    "\n",
    "# Replace the county names with borough names in the 'County' column\n",
    "crimes['County'] = crimes['County'].replace(county_to_borough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c280dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.County.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa1965e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79081c",
   "metadata": {},
   "source": [
    "### Rental Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f38d1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "rental = pd.read_csv(\"./Data/DOF_Condominium_Comparable_Rental_Income_in_NYC_20231211.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20477867",
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_gross_income = rental.groupby(['Neighborhood']).agg({'Gross Income per SqFt': 'mean'})\n",
    "rental_gross_income.sort_index(inplace=True)\n",
    "rental_gross_income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73a78b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_neighbor_price = df2024_01.groupby(['neighbourhood']).agg({'price': 'mean'})\n",
    "airbnb_neighbor_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "387b39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_neighbor_price.index = airbnb_neighbor_price.index.str.upper()\n",
    "filtered_rental_gross_income = rental_gross_income[rental_gross_income.index.isin(airbnb_neighbor_price.index)]\n",
    "filtered_rental_gross_income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "975114f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rental_marketvalue = rental.groupby(['Neighborhood']).agg({'Market Value per SqFt': 'mean'})\n",
    "filtered_Rental_marketvalue = Rental_marketvalue[Rental_marketvalue.index.isin(airbnb_neighbor_price.index)]\n",
    "filtered_Rental_marketvalue.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388d07f",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4371a",
   "metadata": {},
   "source": [
    "(目前只做了202401 后面需要可以直接copy代码改数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f4e3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary columns\n",
    "df2024_01.drop(['name','id','host_name','last_review','number_of_reviews_ltm', 'license'], axis=1, inplace=True)\n",
    "df2024_01.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58c97fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the number columns\n",
    "numeric_df = df2024_01.select_dtypes(include=[np.number])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea632b",
   "metadata": {},
   "source": [
    "Get Correlation between different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e5431bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = numeric_df.corr(method='kendall')\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d2e1a7",
   "metadata": {},
   "source": [
    "Plot all Neighbourhood Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7148fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='neighbourhood_group', data=df2024_01, palette=\"plasma\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,10)\n",
    "plt.title('Neighbourhood Group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755754c",
   "metadata": {},
   "source": [
    "Neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15f91230",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='neighbourhood', data=df2024_01, palette=\"plasma\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(25,6)\n",
    "plt.title('Neighbourhood')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7964a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2024_01['neighbourhood'] = df2024_01['neighbourhood'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c431d",
   "metadata": {},
   "source": [
    "Room Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad19c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restaurants delivering Online or not\n",
    "sns.countplot(x='room_type', data=df2024_01, palette=\"plasma\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,10)\n",
    "plt.title('Restaurants delivering online or Not')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d6fbf",
   "metadata": {},
   "source": [
    "Relation between neighbourgroup and Availability of Room¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c1a560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.boxplot(data=df2024_01, x='neighbourhood_group',y='availability_365',palette='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "70804568",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x=df2024_01['longitude'], y=df2024_01['latitude'], hue=df2024_01['neighbourhood_group'])\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29af5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x=df2024_01['longitude'], y=df2024_01['latitude'], hue=df2024_01['availability_365'])\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "76dc93bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2024_01.head()\n",
    "#发现availability_365数据有问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81176605",
   "metadata": {},
   "source": [
    "### Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e91e6b",
   "metadata": {},
   "source": [
    "### PCA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7751f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2024_01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9bf330",
   "metadata": {},
   "source": [
    "## Nueral Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87acb0a6",
   "metadata": {},
   "source": [
    "## Concatenate all the airbnb data with zip from 2023-04 to 2024-02 as one dataframe\n",
    "### Later, we will splite this dataset as Train and Validation set\n",
    "### Then, we will treat 2024-03 as the test set (or make predictions on them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b8273697",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([airbnb202304_with_zip, \n",
    "                           airbnb202305_with_zip, \n",
    "                           airbnb202306_with_zip, \n",
    "                           airbnb202307_with_zip,\n",
    "                           airbnb202308_with_zip, \n",
    "                           airbnb202309_with_zip, \n",
    "                           airbnb202310_with_zip, \n",
    "                           airbnb202311_with_zip,\n",
    "                           airbnb202312_with_zip,\n",
    "                           airbnb202401_with_zip,\n",
    "                           airbnb202402_with_zip, \n",
    "                           airbnb202403_with_zip], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c23439e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_data = airbnb202402_with_zip\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67c86bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c578b532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data.columns, combined_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e2be311",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.rename(columns={'neighbourhood_group': 'borough'}, inplace=True)\n",
    "combined_data.last_review = pd.to_datetime(combined_data.last_review )\n",
    "combined_data['days_since_last_review'] = (pd.to_datetime('today') - combined_data['last_review']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de0451ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.drop([\"license\"], axis = 1)\n",
    "combined_data = combined_data.drop([\"id\"], axis = 1)\n",
    "combined_data = combined_data.drop([\"host_id\"], axis = 1)\n",
    "combined_data = combined_data.drop([\"last_review\"], axis = 1)\n",
    "combined_data = combined_data.drop([\"number_of_reviews_ltm\"], axis = 1)\n",
    "# combined_data = combined_data.drop([\"host_name\"], axis = 1)\n",
    "# combined_data = combined_data.drop([\"name\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b40302b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7ee7b33c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9476a2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead8eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13e38b3c",
   "metadata": {},
   "source": [
    "## indicating from which indices on, it is train or test, as we are going to combine them, and then we will split them in to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3e9dc92a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "83169a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = combined_data['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1a7d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3e9b94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.drop(columns=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3631fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "09b1055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9b65a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot \n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "categorical_features = encoder.fit_transform(combined_data[['room_type', 'neighbourhood', 'borough', 'ZIPCODE']])\n",
    "\n",
    "# ompute missing values in numerical columns before scaling\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "numerical_columns = ['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'days_since_last_review']\n",
    "numerical_features = imputer.fit_transform(combined_data[numerical_columns])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_features_scaled = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# combine\n",
    "X = np.hstack((categorical_features, numerical_features_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0ff6e200",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encoder = OneHotEncoder(sparse_output=True)\n",
    "# categorical_features = encoder.fit_transform(combined_data[['room_type', 'neighbourhood', 'borough', 'ZIPCODE']])\n",
    "# categorical_features_dense = categorical_features.toarray()\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# numerical_columns = ['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'days_since_last_review']\n",
    "# numerical_features = scaler.fit_transform(combined_data[numerical_columns])\n",
    "\n",
    "# X = np.hstack((categorical_features_dense, numerical_features))\n",
    "\n",
    "# print('Shape of X (features):', X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "36468f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "182faec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "042ff06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality reduction with PCA\n",
    "# pca = PCA(n_components=100)\n",
    "# X = pca.fit_transform(X)\n",
    "# X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c2d11244",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d5a7750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality reduction with PCA\n",
    "# pca = PCA(n_components=100)\n",
    "# X = pca.fit_transform(X)\n",
    "# X_val = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8a3d58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d151e350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.shape, X_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2c96fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1ac7cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df = pd.DataFrame(X_train)\n",
    "# X_train_filled = X_train_df.fillna(X_train_df.mean())\n",
    "# X_train_filled_array = X_train_filled.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "78cc0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train_filled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "873a1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_series = pd.Series(y_train)\n",
    "# y_train_filled = y_train_series.fillna(y_train_series.mean())\n",
    "# y_train_filled_array = y_train_filled.values\n",
    "# y_train = y_train_filled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "18e2b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val_df = pd.DataFrame(y_val)\n",
    "# y_val_filled = y_val_df.fillna(y_val_df.mean())\n",
    "# y_val_filled_array = y_val_filled.values\n",
    "# y_val = y_val_filled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6c21dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_df = pd.DataFrame(X_val)\n",
    "# X_val_filled = X_val_df.fillna(X_val_df.mean())\n",
    "# X_val_filled_array = X_val_filled.values\n",
    "# X_val = X_val_filled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "44e0f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to fill nan\n",
    "X_train = np.nan_to_num(X_train, nan=np.nanmean(X_train))\n",
    "X_val= np.nan_to_num(X_val, nan=np.nanmean(X_val))\n",
    "# X_test_filled = np.nan_to_num(X_test, nan=np.nanmean(X_test))\n",
    "\n",
    "y_train = np.nan_to_num(y_train, nan=np.nanmean(y_train))\n",
    "y_val = np.nan_to_num(y_val, nan=np.nanmean(y_val))\n",
    "# y_test_filled = np.nan_to_num(y_test, nan=np.nanmean(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2f03010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  \n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)    \n",
    "# y_test_tensor = torch.tensor(y_test_filled, dtype=torch.float32).view(-1, 1)  \n",
    "\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
    "# test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c0edb8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "925c72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "75f2b023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
    "print(\"y_train_tensor shape:\", y_train_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b0f6f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_tensor shape:\", X_val_tensor.shape)\n",
    "print(\"y_train_tensor shape:\", y_val_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "78cac594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "be2ecce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# math(-3.0129e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a89a9042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbnbPricePredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(AirbnbPricePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)  # Increased hidden layer size\n",
    "        self.fc2 = nn.Linear(256, 128)         # Increased hidden layer size\n",
    "        self.fc3 = nn.Linear(128, 64)          # Increased hidden layer size\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class AirbnbPricePredictor(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super(AirbnbPricePredictor, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 256)\n",
    "#         self.bn1 = nn.BatchNorm1d(256)  \n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.bn2 = nn.BatchNorm1d(128)  \n",
    "#         self.fc3 = nn.Linear(128, 64)\n",
    "#         self.bn3 = nn.BatchNorm1d(64)   \n",
    "#         self.fc4 = nn.Linear(64, 32)\n",
    "#         self.fc5 = nn.Linear(32, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.bn1(self.fc1(x)))  \n",
    "#         x = F.relu(self.bn2(self.fc2(x))) \n",
    "#         x = F.relu(self.bn3(self.fc3(x)))  \n",
    "#         x = F.relu(self.fc4(x))\n",
    "#         x = self.fc5(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2a8580df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# # Assuming model and other components are already defined and initialized\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0002)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)\n",
    "\n",
    "# epochs = 200\n",
    "# best_val_loss = float('inf')\n",
    "# threshold = 0.1  # Define a threshold for accuracy (e.g., 10%)\n",
    "\n",
    "# def calculate_accuracy(outputs, labels, threshold):\n",
    "#     abs_diff = torch.abs(outputs - labels)\n",
    "#     allowed_diff = threshold * torch.abs(labels)\n",
    "#     return torch.mean((abs_diff <= allowed_diff).float()).item()\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "#     train_acc = 0.0\n",
    "#     total_samples = 0\n",
    "\n",
    "#     for inputs, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item() * inputs.size(0)\n",
    "#         train_acc += calculate_accuracy(outputs, labels, threshold) * inputs.size(0)\n",
    "#         total_samples += inputs.size(0)\n",
    "\n",
    "#     train_loss /= total_samples\n",
    "#     train_acc /= total_samples\n",
    "\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     val_acc = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             val_loss += loss.item() * inputs.size(0)\n",
    "#             val_acc += calculate_accuracy(outputs, labels, threshold) * inputs.size(0)\n",
    "\n",
    "#     val_loss /= len(val_loader.dataset)\n",
    "#     val_acc /= len(val_loader.dataset)\n",
    "\n",
    "#     print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2%}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2%}')\n",
    "#     scheduler.step(val_loss)\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         # Save model checkpoint here if needed\n",
    "#     else:\n",
    "#         print(\"No improvement in validation loss, consider stopping training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0e6f6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "model = AirbnbPricePredictor(input_size)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002)\n",
    "# scheduler = StepLR(optimizer, step_size=50, gamma=0.1)  # Adjust step_size and gamma as needed\n",
    "\n",
    "epochs = 200\n",
    "threshold = 0.1  # Define a threshold for accuracy (e.g., 10%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "model = AirbnbPricePredictor(input_size)\n",
    "criterion = nn.L1Loss(reduction='mean')  # Using L1Loss for Mean Absolute Error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002)\n",
    "# scheduler = StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "epochs = 200\n",
    "threshold = 0.1  # 10% threshold\n",
    "\n",
    "def calculate_accuracy(outputs, labels, threshold):\n",
    "    abs_diff = torch.abs(outputs - labels)\n",
    "    allowed_diff = threshold * torch.abs(labels)\n",
    "    return torch.mean((abs_diff <= allowed_diff).float()).item()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        train_accuracy += calculate_accuracy(outputs, labels, threshold) * inputs.size(0)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_train_accuracy = train_accuracy / total_samples\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            val_accuracy += calculate_accuracy(outputs, labels, threshold) * inputs.size(0)\n",
    "            \n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "    epoch_val_accuracy = val_accuracy / len(val_loader.dataset)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Training MAE: {epoch_loss:.4f}, Train Acc: {epoch_train_accuracy:.2%}, Validation MAE: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39960e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate the accuracy-like metric\n",
    "        with torch.no_grad():\n",
    "            abs_percentage_error = torch.abs((outputs - labels) / labels)\n",
    "            correct_predictions += torch.sum(abs_percentage_error < threshold).item()\n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            \n",
    "    epoch_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Train Acc: {train_accuracy:.2%}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "print(input_size)\n",
    "model = AirbnbPricePredictor(input_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002)\n",
    "\n",
    "# optimizer = optim.Adamax(model.parameters(), lr=0.002, betas=(0.9, 0.999))\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Define cyclic learning rate scheduler\n",
    "# step_size_up = 100  # Number of steps for the learning rate to increase\n",
    "# step_size_down = 200  # Number of steps for the learning rate to decrease\n",
    "# base_lr = 0.001  # Lower boundary for learning rate\n",
    "# max_lr = 0.01  # Upper boundary for learning rate\n",
    "# clr_scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "#                          step_size_up=step_size_up, step_size_down=step_size_down)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cafe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "\n",
    "# Training loop\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Check for NaN values in inputs\n",
    "        if np.isnan(inputs).any():\n",
    "            print(\"NaN values detected in inputs!\")\n",
    "            # Handle NaN values appropriately\n",
    "            \n",
    "        # Check for NaN values in labels\n",
    "        if np.isnan(labels).any():\n",
    "            print(\"NaN values detected in labels!\")\n",
    "            # Handle NaN values appropriately\n",
    "            \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Calculate absolute percentage error for accuracy-like metric\n",
    "        abs_percentage_error = torch.abs(outputs - labels) / labels\n",
    "        \n",
    "        # Count correct predictions within the threshold\n",
    "        correct_predictions += torch.sum(abs_percentage_error <= threshold)\n",
    "        total_predictions += len(labels)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Accuracy: {accuracy:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18462da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "threshold = 0.1\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions_train = 0\n",
    "    total_predictions_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "#         print(inputs)\n",
    "#         break\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "#         print(\"outputs\", outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print(\"labels\", labels)\n",
    "        \n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate training loss\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # calculate absolute percentage error for training accuracy-like metric\n",
    "        abs_percentage_error_train = torch.abs(outputs - labels) / labels\n",
    "#         print(\"abs_percentage_error_train\", abs_percentage_error_train)\n",
    "        \n",
    "        # count correct predictions within the threshold for training\n",
    "        correct_predictions_train += torch.sum(abs_percentage_error_train <= threshold)\n",
    "#         print(\"correct_predictions_train\", correct_predictions_train)\n",
    "        total_predictions_train += len(labels)\n",
    "#         print(\"total_predictions_train\", total_predictions_train)\n",
    "        \n",
    "        \n",
    "    #training accuracy\n",
    "    accuracy_train = correct_predictions_train/ total_predictions_train\n",
    "\n",
    "    \n",
    "    #training loss\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    #evaluation on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_predictions_val = 0\n",
    "    total_predictions_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            abs_percentage_error_val = torch.abs(outputs - labels) / labels\n",
    "            \n",
    "            correct_predictions_val += torch.sum(abs_percentage_error_val <= threshold)\n",
    "            total_predictions_val += len(labels)\n",
    "    \n",
    "    # validation loss and accuracy\n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "    accuracy_val = correct_predictions_val.float() / total_predictions_val\n",
    "    \n",
    "    # metrics\n",
    "    print(f'Epoch {epoch+1}/{epochs}, '\n",
    "          f'Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy_train:.2%}, '\n",
    "          f'Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {accuracy_val:.2%}, '\n",
    "         )\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8ff80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
